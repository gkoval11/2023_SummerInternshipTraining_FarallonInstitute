{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Example: Satellite Data \n",
    "###  Days with sea surface temperature above a threshold\n",
    "\n",
    "In this chapter we exemplify the use of Sea Surface Temperature (SST) data in the cloud. \n",
    "\n",
    "This example analyzes a time series from an area of the ocean or a point. If an area, it averages SST values into a single value. Then it analyze the time series to assess when SST is above a given threshold. This could be used to study marine heatwaves, or use a SST threshold relevant to a marine species of interest.\n",
    "\n",
    "<span style=\"font-size:larger;\">__You must have the Zarr package installed as well__</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime as dt\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import warnings \n",
    "warnings.simplefilter('ignore') \n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from calendar import month_abbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "\n",
    "# select either a range of lat/lon or a point. \n",
    "# If a point, set both entries to the same value\n",
    "latr = [35, 40] # make sure lat1 < lat2 since no test is done below to simplify the code\n",
    "lonr = [-125, -120] # lon1 < lon2, range -180:180. resolution daily 1km!\n",
    "\n",
    "# time range. data range available: 2002-06-01 to 2020-01-20. [start with a short period]\n",
    "dater = ['2018-01-06','2018-01-14'] # dates on the format 'YYYY-MM-DD' as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## We are going to use the Multi-Scale Ultra High Resolution (MUR) Sea Surface Temperature (SST) data set\n",
    "### This dataset is stored the Amazon (AWS) Cloud. For more info and links to the data detail and examples, see: https://registry.opendata.aws/mur/\n",
    "\n",
    "This dataset is stored in `zarr` format, which is an optimized format for the large datasets and the cloud. It is not stored as one 'image' at a time or a gigantic netcdf file, but in 'chunks', so it is perfect for extracting time series.\n",
    "\n",
    "First, we open the dataset and explore it, but we are not downloading anything yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first determine the file name using, in the format:\n",
    "# the s3 bucket [mur-sst], and the region [us-west-2], and the folder if applicable [zarr-v1] \n",
    "file_location = 'https://mur-sst.s3.us-west-2.amazonaws.com/zarr-v1'\n",
    "\n",
    "ds_sst = xr.open_zarr(file_location,consolidated=True) # open a zarr file using xarray\n",
    "# it is similar to open_dataset but it only reads the metadata\n",
    "\n",
    "ds_sst # we can treat it as a dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we know what the file contains, we select our data (region and time), operate on it if needed (if a region, average), and download only the selected data \n",
    "It takes a while given the high resolution of the data. So, be patient.... and if you're only testing, might want to choose a small region and a short time period first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all values that are for lakes (look at the meta data for the mask field above)\n",
    "sst_filtered = ds_sst.where(ds_sst.mask != 5, np.nan)\n",
    "\n",
    "#filter the data using the specified extent of the latitutde, longitude, and time from above\n",
    "sst = sst_filtered['analysed_sst'].sel(time = slice(dater[0],dater[1]),\n",
    "                        lat  = slice(latr[0], latr[1]), \n",
    "                        lon  = slice(lonr[0], lonr[1])\n",
    "                        ).mean(dim={'time'}, skipna=True, keep_attrs=True).load() # skip 'not a number' (NaN) values and keep attributes\n",
    "\n",
    "sst = sst-273.15 # transform units from Kelvin to Celsius\n",
    "sst.attrs['units']='deg C' # update units in metadata\n",
    "sst.to_netcdf('data/sst_example.nc') # saving the data, incase we want to come back to analyze the same data, but don't want to acquire it again from the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### *Execute the next cell only if your reading the data from a file - either no access to cloud, or not want to keep reading from it. Skip otherwise. (No problem if you executed it by mistake).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the temperature data and close it \n",
    "sst = xr.open_dataset('data/sst_example.nc') \n",
    "sst.close()\n",
    "\n",
    "#look at the temperature data\n",
    "sst.analysed_sst.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define latitude and longitude boundaries\n",
    "latr = [np.nanmin(sst.analysed_sst['lat']), np.nanmax(sst.analysed_sst['lat'])] \n",
    "lonr = [np.nanmax(sst.analysed_sst['lon']), np.nanmin(sst.analysed_sst['lon'])] \n",
    "\n",
    "# Select a region of our data, giving it a margin\n",
    "margin = 0\n",
    "region = np.array([[latr[0]-margin,latr[1]+margin],[lonr[0]+margin,lonr[1]-margin]]) \n",
    "\n",
    "#add state outlines\n",
    "states_provinces = cfeature.NaturalEarthFeature(\n",
    "        category='cultural',\n",
    "        name='admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none')\n",
    "\n",
    "# Create and set the figure context\n",
    "fig = plt.figure(figsize=(16,10), dpi = 72) \n",
    "ax = plt.axes(projection=ccrs.PlateCarree()) \n",
    "ax.coastlines(resolution='10m',linewidth=1,color='black') \n",
    "ax.add_feature(cfeature.LAND, color='grey', alpha=0.3)\n",
    "ax.add_feature(states_provinces, linewidth = 0.5)\n",
    "ax.add_feature(cfeature.BORDERS, color = 'black')\n",
    "ax.set_extent([region[1,0],region[1,1],region[0,0],region[0,1]],crs=ccrs.PlateCarree()) \n",
    "ax.set_xticks(np.round([*np.arange(region[1,1],region[1,0]+1,1)][::-1],0), crs=ccrs.PlateCarree()) \n",
    "ax.set_yticks(np.round([*np.arange(np.floor(region[0,0]),region[0,1]+1,1)],1), crs=ccrs.PlateCarree()) \n",
    "ax.xaxis.set_major_formatter(LongitudeFormatter(zero_direction_label=True))\n",
    "ax.yaxis.set_major_formatter(LatitudeFormatter())\n",
    "ax.gridlines(linestyle = '--', linewidth = 0.5)\n",
    "\n",
    "# Plot track data, color by temperature\n",
    "sst.analysed_sst.plot(transform=ccrs.PlateCarree(),cbar_kwargs={'label': 'Temperature [deg C]'}, cmap = \"RdBu_r\")\n",
    "plt.title('Averaged Temperature Values ('+dater[0]+' to '+dater[1]+')', fontdict = {'fontsize' : 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Switching to the Salinity Data from the Soil Moisture Active Passive (SMAP) observatory through JPL\n",
    "\n",
    "For this exercise we'll be use the level 3 collocated salinity data. You can access the data through a direct S3 access through Amazon Web Services (AWS) of selecting the granule data in the Earth Explorer app. The data is averaged by 8 day segments. In this exercise instead of gathering the data through the direct S3 access, you have been provided a single file of the eight day averaged data. The file below contains data from 2018-01-06 to 2018-01-14.\n",
    "\n",
    "[JPL SMAP Sea Surface Salnity](https://podaac.jpl.nasa.gov/dataset/SMAP_JPL_L3_SSS_CAP_8DAY-RUNNINGMEAN_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the salinity dataset and close it \n",
    "sss = xr.open_dataset('data/RSS_smap_SSS_L3_8day_running_2018_006_FNL_v04.0.nc4') \n",
    "sss.close()\n",
    "\n",
    "#look at the variable containing the salinity data\n",
    "sss.sss_smap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us look at the map of the whole world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss.sss_smap.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's zoom in to a the West Coast of the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create boundaries for the latitute and longitude\n",
    "mask_lon = (sss.lon >= 210) & (sss.lon <= 250)\n",
    "mask_lat = (sss.lat >= 20) & (sss.lat <= 70)\n",
    "\n",
    "#filter the data using .where() and the two variables we just created\n",
    "sss_zoomed = sss.where(mask_lon & mask_lat, drop=True)\n",
    "\n",
    "#show the map of it \n",
    "sss_zoomed.sss_smap.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SST and SSS Data\n",
    "\n",
    "Lets make two figures that show the spatial differences between the temperature and salinity. Notice 1) the differences in spatial resoluation (the size of the pixels for each data point), and 2) the distance from shore that temperature and salinity are measured to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Temperature Map\n",
    "#create the figure\n",
    "plt.figure(figsize=(16,8), dpi = 72)\n",
    "p = sst.analysed_sst.plot(\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree()),\n",
    "    transform=ccrs.PlateCarree())\n",
    "plt.title(\"Temperature\")\n",
    "p.axes.coastlines()\n",
    "\n",
    "###Salinity Map\n",
    "#set up variables to filter the latitutde and longitude for the salinity\n",
    "sss_mask_lon = (sss.lon >= 235) & (sss.lon <= 240)\n",
    "sss_mask_lat = (sss.lat >= 35) & (sss.lat <= 40)\n",
    "\n",
    "#creatrue the figure\n",
    "plt.figure(figsize=(16,8), dpi = 72)\n",
    "sss_compare = sss.where(sss_mask_lon & sss_mask_lat, drop=True)\n",
    "q = sss_compare.sss_smap.plot(\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree()),\n",
    "    cmap = 'plasma',\n",
    "    transform=ccrs.PlateCarree())\n",
    "plt.title(\"Salinity\")\n",
    "q.axes.coastlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolating the Salinity Data\n",
    "\n",
    "Interpolating data is very valuable given the large differences in the resolution of the salinity and temperature data. If we want to accurately compare these data, we need interpolate the values of the salinity data to match the resolution of the temperature data. First you need to match up the longitude degrees. The sea surface temperature data uses negative degrees to represent longitude values west of the prime meridian, so the values range from -180 to 180. The salinity data only uses values east of the prime meridian, so the values range from 0 to 360. Because of these differences we first need to transform the longitude values to match each other. \n",
    "\n",
    "Once the longitudes have been corrected, the salinity data can then be interpolated using the .interp_like() function which takes the array you want to interpolate and transforms it to the array you want. This is a 2D interpolation, so it interpolates both latitude and longitude (or two other variables). Another common use of this function is to interpolation data through time instead of the space. \n",
    "\n",
    "Looking at the map of the interpolated data, we can see that the resolution of the salinity data now matches the temperature data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the longitudes to be from 0-360 to -180-180\n",
    "sss_transform = sss_compare.sortby(sss_compare.lon)\n",
    "sss_transform.coords['lon'] = np.mod(sss_transform.coords['lon'] + 180,360) - 180\n",
    "\n",
    "#interpolate the data using the .interp_like() function\n",
    "sss_inter = sss_transform.interp_like(sst.analysed_sst)\n",
    "\n",
    "#look at the shape of the data before and after the interpolation\n",
    "print(\"Shape before interpolation: \", sss_compare.dims)\n",
    "print(\"Shape after interpolation: \", sss_inter.dims)\n",
    "\n",
    "#view the new interpolated data\n",
    "plt.figure(figsize=(16,8), dpi = 72)\n",
    "q = sss_inter.sss_smap.plot(\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree()),\n",
    "    cmap = 'plasma',\n",
    "    transform=ccrs.PlateCarree())\n",
    "plt.title(\"Interpolated Salinity\")\n",
    "q.axes.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "### Resources specifically for this chapter:\n",
    "\n",
    "- [MUR SST Data](https://registry.opendata.aws/mur/). SST data in the cloud, with references the official datta website, examples and other resources.\n",
    "\n",
    "- [Pangeo OSM2020 Tutorial](https://github.com/pangeo-gallery/osm2020tutorial). This is a very good tutorial for ocean application and cloud computing. Plenty of examples. Many of the commands here are from this tutorial.\n",
    "\n",
    "### If you want to learn more:\n",
    "\n",
    "- [Methods for accessing a AWS bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-bucket-intro.html). Bucket is the name of the cloud storage object. S3 stands for Amazon's Simple Storage Service.\n",
    "\n",
    "- [hvplot site](https://hvplot.holoviz.org/index.html). Plotting tool used here.\n",
    "\n",
    "- [zarr](https://zarr.readthedocs.io/en/stable/). Learn more about this big data storage format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
